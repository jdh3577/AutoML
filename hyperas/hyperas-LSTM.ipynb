{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "#-------------------------- set gpu using tf ---------------------------\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\n",
    "config = tf.ConfigProto(gpu_options=gpu_options)\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "#-------------------  start importing keras module ---------------------\n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "import tensorflow as tf\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "from __future__ import print_function\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras.backend.tensorflow_backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing import sequence\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import imdb\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.embeddings import Embedding\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.recurrent import LSTM\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: maxlen = 100\n",
      "  3: max_features = 20000\n",
      "  4: \n",
      "  5: print('Loading data...')\n",
      "  6: (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
      "  7: print(len(X_train), 'train sequences')\n",
      "  8: print(len(X_test), 'test sequences')\n",
      "  9: \n",
      " 10: print(\"Pad sequences (samples x time)\")\n",
      " 11: X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
      " 12: X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
      " 13: print('X_train shape:', X_train.shape)\n",
      " 14: print('X_test shape:', X_test.shape)\n",
      " 15: \n",
      " 16: \n",
      " 17: \n",
      " 18: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "  1: def keras_fmin_fnct(space):\n",
      "  2: \n",
      "  3:     model = Sequential()\n",
      "  4:     model.add(Embedding(max_features, 128, input_length=maxlen))\n",
      "  5:     model.add(LSTM(128))\n",
      "  6:     model.add(Dropout(space['Dropout']))\n",
      "  7:     model.add(Dense(1))\n",
      "  8:     model.add(Activation('sigmoid'))\n",
      "  9: \n",
      " 10:     model.compile(loss='binary_crossentropy',\n",
      " 11:                   optimizer='adam',\n",
      " 12:                   metrics=['accuracy'])\n",
      " 13: \n",
      " 14:     early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
      " 15:     checkpointer = ModelCheckpoint(filepath='keras_weights.hdf5',\n",
      " 16:                                    verbose=1,\n",
      " 17:                                    save_best_only=True)\n",
      " 18: \n",
      " 19:     model.fit(X_train, y_train,\n",
      " 20:               batch_size=space['batch_size'],\n",
      " 21:               nb_epoch=1,\n",
      " 22:               validation_split=0.08,\n",
      " 23:               callbacks=[early_stopping, checkpointer])\n",
      " 24: \n",
      " 25:     score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
      " 26: \n",
      " 27:     print('Test accuracy:', acc)\n",
      " 28:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      " 29: \n",
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jinhak\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\datasets\\imdb.py:44: UserWarning: The `nb_words` argument in `load_data` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `load_data` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (25000, 100)\n",
      "X_test shape: (25000, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jinhak\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "22912/23000 [============================>.] - ETA: 0s - loss: 0.4319 - acc: 0.7912Epoch 00001: val_loss improved from inf to 0.35650, saving model to keras_weights.hdf5\n",
      "23000/23000 [==============================] - 24s 1ms/step - loss: 0.4313 - acc: 0.7915 - val_loss: 0.3565 - val_acc: 0.8405\n",
      "Test accuracy: 0.8444\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "22976/23000 [============================>.] - ETA: 0s - loss: 0.4251 - acc: 0.7992Epoch 00001: val_loss improved from inf to 0.35733, saving model to keras_weights.hdf5\n",
      "23000/23000 [==============================] - 100s 4ms/step - loss: 0.4250 - acc: 0.7993 - val_loss: 0.3573 - val_acc: 0.8510\n",
      "Test accuracy: 0.84732\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "22976/23000 [============================>.] - ETA: 0s - loss: 0.4427 - acc: 0.7865Epoch 00001: val_loss improved from inf to 0.36068, saving model to keras_weights.hdf5\n",
      "23000/23000 [==============================] - 47s 2ms/step - loss: 0.4428 - acc: 0.7865 - val_loss: 0.3607 - val_acc: 0.8405\n",
      "Test accuracy: 0.84436\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "22976/23000 [============================>.] - ETA: 0s - loss: 0.4315 - acc: 0.7960Epoch 00001: val_loss improved from inf to 0.35533, saving model to keras_weights.hdf5\n",
      "23000/23000 [==============================] - 47s 2ms/step - loss: 0.4315 - acc: 0.7960 - val_loss: 0.3553 - val_acc: 0.8455\n",
      "Test accuracy: 0.8458\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "22976/23000 [============================>.] - ETA: 0s - loss: 0.4313 - acc: 0.7976Epoch 00001: val_loss improved from inf to 0.34561, saving model to keras_weights.hdf5\n",
      "23000/23000 [==============================] - 101s 4ms/step - loss: 0.4313 - acc: 0.7976 - val_loss: 0.3456 - val_acc: 0.8485\n",
      "Test accuracy: 0.8492\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "22912/23000 [============================>.] - ETA: 0s - loss: 0.4525 - acc: 0.7751Epoch 00001: val_loss improved from inf to 0.33727, saving model to keras_weights.hdf5\n",
      "23000/23000 [==============================] - 24s 1ms/step - loss: 0.4519 - acc: 0.7753 - val_loss: 0.3373 - val_acc: 0.8565\n",
      "Test accuracy: 0.8536\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "22976/23000 [============================>.] - ETA: 0s - loss: 0.4237 - acc: 0.7959Epoch 00001: val_loss improved from inf to 0.34523, saving model to keras_weights.hdf5\n",
      "23000/23000 [==============================] - 101s 4ms/step - loss: 0.4235 - acc: 0.7960 - val_loss: 0.3452 - val_acc: 0.8540\n",
      "Test accuracy: 0.84872\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "22912/23000 [============================>.] - ETA: 0s - loss: 0.4582 - acc: 0.7740Epoch 00001: val_loss improved from inf to 0.35620, saving model to keras_weights.hdf5\n",
      "23000/23000 [==============================] - 24s 1ms/step - loss: 0.4577 - acc: 0.7744 - val_loss: 0.3562 - val_acc: 0.8445\n",
      "Test accuracy: 0.84768\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "22976/23000 [============================>.] - ETA: 0s - loss: 0.4330 - acc: 0.7982Epoch 00001: val_loss improved from inf to 0.35301, saving model to keras_weights.hdf5\n",
      "23000/23000 [==============================] - 102s 4ms/step - loss: 0.4329 - acc: 0.7982 - val_loss: 0.3530 - val_acc: 0.8465\n",
      "Test accuracy: 0.84472\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "22976/23000 [============================>.] - ETA: 0s - loss: 0.4323 - acc: 0.7989Epoch 00001: val_loss improved from inf to 0.35396, saving model to keras_weights.hdf5\n",
      "23000/23000 [==============================] - 48s 2ms/step - loss: 0.4323 - acc: 0.7989 - val_loss: 0.3540 - val_acc: 0.8445\n",
      "Test accuracy: 0.84892\n",
      "{'batch_size': 2, 'Dropout': 0.16046181714460095}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def data():\n",
    "    maxlen = 100\n",
    "    max_features = 20000\n",
    "\n",
    "    print('Loading data...')\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "    print(len(X_train), 'train sequences')\n",
    "    print(len(X_test), 'test sequences')\n",
    "\n",
    "    print(\"Pad sequences (samples x time)\")\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print('X_test shape:', X_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, max_features, maxlen\n",
    "\n",
    "\n",
    "def model(X_train, X_test, y_train, y_test, max_features, maxlen):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "    checkpointer = ModelCheckpoint(filepath='keras_weights.hdf5',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True)\n",
    "\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size={{choice([32, 64, 128])}},\n",
    "              nb_epoch=1,\n",
    "              validation_split=0.08,\n",
    "              callbacks=[early_stopping, checkpointer])\n",
    "\n",
    "    score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    best_run, best_model = optim.minimize(model=model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=10,\n",
    "                                          trials=Trials(),\n",
    "                                         notebook_name='hyperas-test')\n",
    "\n",
    "    print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
